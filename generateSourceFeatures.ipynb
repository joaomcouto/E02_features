{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import pexpect\n",
    "import sys\n",
    "import time\n",
    "import tldextract\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-03c44ec05a5b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-03c44ec05a5b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    usuarioBlackbird =\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "usuarioBlackbird = \n",
    "senhaBlackbird = \n",
    "filesBlackbird = [#'/scratch4/leandro/data/fn/pickles/top-10-perserver-20210817.csv',\n",
    "                  '/scratch4/leandro/data/fn/pickles/top-10-perserver-20210825.csv',\n",
    "                   '/scratch4/leandro/data/fn/compiled_trusty_news_portals.txt',\n",
    "                   '/scratch4/leandro/data/fn/url2title.csv']\n",
    "destinationLocal = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filesBlackbird' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e4910d36659b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Baixa os arquivos listados em filesBlackbird APENAS SE já não estiverem no diretorio /data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilesBlackbird\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filesBlackbird' is not defined"
     ]
    }
   ],
   "source": [
    "#Baixa os arquivos listados em filesBlackbird APENAS SE já não estiverem no diretorio /data\n",
    "for file in filesBlackbird:\n",
    "    if not os.path.isfile('data/' + file.split('/')[-1]):\n",
    "        child = pexpect.spawn(\"bash\")\n",
    "        child.timeout= 300\n",
    "        child.sendline(\"scp -P2512 -r \" +\n",
    "                              usuarioBlackbird+\n",
    "                              \"@blackbird.dcc.ufmg.br:\"+\n",
    "                              file+ \n",
    "                              \" \"+\n",
    "                             destinationLocal)\n",
    "        child.logfile_read = sys.stdout\n",
    "        child.expect(\"password:\")\n",
    "        child.sendline(senhaBlackbird)\n",
    "        child.expect(\"100%\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        child.close() \n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega as URLs das 10 noticias mais compartilhadas dos sites de baixa credibilidade em um dataframe\n",
    "dfUrlFalse = pd.read_csv('data/top-10-perserver-20210825.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2045, 3)\n",
      "            amount\n",
      "count  2045.000000\n",
      "mean     55.547677\n",
      "std     112.857622\n",
      "min       1.000000\n",
      "25%       8.000000\n",
      "50%      23.000000\n",
      "75%      61.000000\n",
      "max    2910.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2045 entries, 0 to 2044\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   server  2045 non-null   object\n",
      " 1   url     2045 non-null   object\n",
      " 2   amount  2045 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 48.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>server</th>\n",
       "      <th>url</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/31794/urgente-comandantes-do-alto-patamar-das-forcas-armadas-se-unem-e-lancam-forte-nota-contra-o-tse-equottem-algo-a-esconderequot</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/16821/a-pec-que-extingue-a-pec-da-bengala-ja-esta-na-ccj-e-hora-da-sociedade-agir</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/15866/governo-zera-tarifas-de-importacao-para-medicamentos-contra-cancer-e-hiv</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/31511/grave-randolfe-convoca-equotexercito-de-stedileequot-o-mst-para-derrubar-bolsonaro-veja-o-video</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/31713/avante-a-mentira-jamais-sobrepujou-a-verdade</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/13444/gasolina-cai-pelo-terceiro-mes-consecutivo-e-inflacao-de-fevereiro-e-a-menor-desde-o-plano-real</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/31382/urgente-advogado-de-daniel-silveira-rompe-o-silencio-lanca-dura-nota-e-dispara-equotmoraes-esta-interferindo-na-camara-e-na-pgrequot</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/10218/finalmente-alguem-escreveu-aquilo-que-todos-que-acreditam-em-lula-precisam-conhecer</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/14804/confianca-e-tudo-scania-hyundai-gm-carrefour-e-honda-anunciam-investimentos-no-brasil</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jornaldacidadeonline.com.br</td>\n",
       "      <td>https://www.jornaldacidadeonline.com.br/noticias/14580/o-parlamento-e-a-causa-da-corrupcao-e-nao-a-consequencia</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        server  \\\n",
       "0  jornaldacidadeonline.com.br   \n",
       "1  jornaldacidadeonline.com.br   \n",
       "2  jornaldacidadeonline.com.br   \n",
       "3  jornaldacidadeonline.com.br   \n",
       "4  jornaldacidadeonline.com.br   \n",
       "5  jornaldacidadeonline.com.br   \n",
       "6  jornaldacidadeonline.com.br   \n",
       "7  jornaldacidadeonline.com.br   \n",
       "8  jornaldacidadeonline.com.br   \n",
       "9  jornaldacidadeonline.com.br   \n",
       "\n",
       "                                                                                                                                                                                           url  \\\n",
       "0         https://www.jornaldacidadeonline.com.br/noticias/31794/urgente-comandantes-do-alto-patamar-das-forcas-armadas-se-unem-e-lancam-forte-nota-contra-o-tse-equottem-algo-a-esconderequot   \n",
       "1                                                           https://www.jornaldacidadeonline.com.br/noticias/16821/a-pec-que-extingue-a-pec-da-bengala-ja-esta-na-ccj-e-hora-da-sociedade-agir   \n",
       "2                                                              https://www.jornaldacidadeonline.com.br/noticias/15866/governo-zera-tarifas-de-importacao-para-medicamentos-contra-cancer-e-hiv   \n",
       "3                                       https://www.jornaldacidadeonline.com.br/noticias/31511/grave-randolfe-convoca-equotexercito-de-stedileequot-o-mst-para-derrubar-bolsonaro-veja-o-video   \n",
       "4                                                                                          https://www.jornaldacidadeonline.com.br/noticias/31713/avante-a-mentira-jamais-sobrepujou-a-verdade   \n",
       "5                                       https://www.jornaldacidadeonline.com.br/noticias/13444/gasolina-cai-pelo-terceiro-mes-consecutivo-e-inflacao-de-fevereiro-e-a-menor-desde-o-plano-real   \n",
       "6  https://www.jornaldacidadeonline.com.br/noticias/31382/urgente-advogado-de-daniel-silveira-rompe-o-silencio-lanca-dura-nota-e-dispara-equotmoraes-esta-interferindo-na-camara-e-na-pgrequot   \n",
       "7                                                   https://www.jornaldacidadeonline.com.br/noticias/10218/finalmente-alguem-escreveu-aquilo-que-todos-que-acreditam-em-lula-precisam-conhecer   \n",
       "8                                                 https://www.jornaldacidadeonline.com.br/noticias/14804/confianca-e-tudo-scania-hyundai-gm-carrefour-e-honda-anunciam-investimentos-no-brasil   \n",
       "9                                                                              https://www.jornaldacidadeonline.com.br/noticias/14580/o-parlamento-e-a-causa-da-corrupcao-e-nao-a-consequencia   \n",
       "\n",
       "   amount  \n",
       "0     592  \n",
       "1     512  \n",
       "2     498  \n",
       "3     493  \n",
       "4     476  \n",
       "5     461  \n",
       "6     441  \n",
       "7     432  \n",
       "8     422  \n",
       "9     421  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfUrlFalse.shape)\n",
    "print(dfUrlFalse.describe())\n",
    "print(dfUrlFalse.info())\n",
    "dfUrlFalse.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega e pre-processa os dominios de alta credibilidade COMO LISTADO na anj https://www.anj.org.br/associados/)\n",
    "#O pre-processamento consiste em:\n",
    "    #(1) Pra cada listagem, transforma em subdominio.dominio.sufixo\n",
    "          #Ou seja, \n",
    "          #bbc.com/portuguese vira bbc.com\n",
    "          #theintercept.com/brasil/ vira theintercept.com\n",
    "          #uol.com.br/ vira uol.com.br\n",
    "    #(2) Para listagens que tem subdominio e.g. g1.globo.com (g1 é subdominio) adiciona o dominio à lista de confiaveis\n",
    "          #Ou seja, \n",
    "          #além de valor.globo.com teremos tambem globo.com na lista de fonte confiaveis\n",
    "    #(3) Ordena a lista e retira duplicatas\n",
    "          #A lista original da ANJ já vem com redundancias e.g. uol.com.br e uol.com.br/\n",
    "\n",
    "with open('data/compiled_trusty_news_portals.txt', 'r') as file :\n",
    "    filedata = file.readlines()\n",
    "\n",
    "extraMainTrustyDomains = []\n",
    "for i,dom in enumerate(filedata):\n",
    "    ext = tldextract.extract(dom)\n",
    "    \n",
    "    if(len(ext.subdomain)>0):\n",
    "        filedata[i] = ext.subdomain + \".\" + ext.domain +'.'+ ext.suffix + \"\\n\"\n",
    "        extraMainTrustyDomains.append(ext.domain +'.'+ ext.suffix + \"\\n\")\n",
    "    else:\n",
    "        filedata[i] = ext.domain +'.'+ ext.suffix + \"\\n\"\n",
    "            \n",
    "filedata = filedata + extraMainTrustyDomains\n",
    "filedata = sorted(list(set(filedata)))\n",
    "    \n",
    "\n",
    "with open('data/compiled_trusty_news_portals.txt', 'w') as file:\n",
    "    file.write(\"\".join(filedata))\n",
    "\n",
    "trustyNewsDomains_ = open(\"data/compiled_trusty_news_portals.txt\", \"r\")\n",
    "trustyNewsDomains = pd.DataFrame([x.strip() for x in trustyNewsDomains_.readlines()], columns=['url'])\n",
    "\n",
    "#Retira sites indesejados\n",
    "trustyUndesirables = ['nytimes', 'bbc','terra', 'intercept'] \n",
    "trustyNewsDomains = trustyNewsDomains.drop(trustyNewsDomains[trustyNewsDomains['url'].str.contains('|'.join(trustyUndesirables))].index)\n",
    "trustyNewsDomains = trustyNewsDomains.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "               url\n",
      "count          100\n",
      "unique         100\n",
      "top     uai.com.br\n",
      "freq             1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     100 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 928.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acidadeon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acritica.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agazetaderondoniadigital.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agorarn.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atarde.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atribuna.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avozdacidade.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bopaper.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>clicrbs.com.br</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            url\n",
       "0                 acidadeon.com\n",
       "1                  acritica.com\n",
       "2  agazetaderondoniadigital.com\n",
       "3                agorarn.com.br\n",
       "4                     an.com.br\n",
       "5                 atarde.com.br\n",
       "6               atribuna.com.br\n",
       "7              avozdacidade.com\n",
       "8                bopaper.com.br\n",
       "9                clicrbs.com.br"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trustyNewsDomains.shape)\n",
    "print(trustyNewsDomains.describe())\n",
    "print(trustyNewsDomains.info())\n",
    "trustyNewsDomains.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega a coluna URL do csv com todas as URLs expandidas pelo leandro e \n",
    "    #pre-processa para pegar apenas as URLs de sites de alta credibilidade\n",
    "    \n",
    "#O pre-processamento consiste em\n",
    "     #(1) Consertar erros no csv: substitui aspas duplas por simples \n",
    "     #(2) Pega as URLs cujo dominio ou subdominio estão na lista da ANJ AMPLIADA conforme descrito acima\n",
    "\n",
    "with open('data/url2title.csv', 'r') as file :\n",
    "    filedata = file.read()\n",
    "filedata = filedata.replace('\"', \"'\")\n",
    "with open('data/url2title.csv', 'w') as file:\n",
    "    file.write(filedata)\n",
    "\n",
    "dfUrlTrue_ = pd.read_csv('data/url2title.csv', delimiter='§', engine='python', usecols=['url'])\n",
    "\n",
    "dfUrlTrue =[]\n",
    "for x in list(dfUrlTrue_['url']):\n",
    "    try:\n",
    "        ext = tldextract.extract(x)\n",
    "    except Exception as e:\n",
    "        #print(\"Deu excessao no x:\", x , \"\\n\")\n",
    "        continue\n",
    "    if(ext.domain +'.'+ ext.suffix in list(trustyNewsDomains['url'])):\n",
    "        dfUrlTrue.append(x)\n",
    "    elif(ext.subdomain + \".\" + ext.domain +'.'+ ext.suffix in list(trustyNewsDomains['url'])):\n",
    "        dfUrlTrue.append(x)\n",
    "        \n",
    "dfUrlTrue = pd.DataFrame(dfUrlTrue, columns = ['url'])\n",
    "\n",
    "#Retira urls com substrings indesejadas\n",
    "urlTrueUndesirables = ['.jpg','index.php','/tag/', '/tudo-sobre/', '/especiais/','/editoria/'] \n",
    "dfUrlTrue = dfUrlTrue.drop(dfUrlTrue[dfUrlTrue['url'].str.contains('|'.join(map(re.escape,urlTrueUndesirables)))].index)\n",
    "dfUrlTrue = dfUrlTrue.reset_index(drop=True)\n",
    "\n",
    "\n",
    "indexesToDrop = []\n",
    "for index, row in dfUrlTrue.iterrows():\n",
    "    url = row['url']\n",
    "    ext = tldextract.extract(url)\n",
    "    \n",
    "    #Retira urls cujo path não tem pelo menos um subdiretorio x tal que /x/ seja composto por letras E numeros. \n",
    "    # Se todos forem apenas letras ou apenas numeros a url é eliminada\n",
    "    try:\n",
    "        pathList = url.split(ext.suffix + '/')[1].split('/')\n",
    "    except Exception as e:\n",
    "        indexesToDrop.append(index)\n",
    "        continue\n",
    "        #print(\"Deu excessao na url:\" , url)\n",
    "        #print(sys.exc_info()[2])\n",
    "        \n",
    "    onlyAlphaOrOnlyDigitCount = 0\n",
    "    for subdir in pathList:\n",
    "        if(subdir.isdigit() or subdir.isalpha()):\n",
    "            onlyAlphaOrOnlyDigitCount += 1\n",
    "    if(url[-1]=='/'):\n",
    "        if(onlyAlphaOrOnlyDigitCount==len(pathList)-1):\n",
    "            indexesToDrop.append(index)\n",
    "            continue\n",
    "    else:\n",
    "        if(onlyAlphaOrOnlyDigitCount==len(pathList)):\n",
    "            indexesToDrop.append(index)\n",
    "            continue\n",
    "            \n",
    "    #Retira urls que são apenas o proprio site     \n",
    "    if (url.endswith((ext.suffix, ext.suffix + \"/\"))):\n",
    "        if(index not in indexesToDrop):\n",
    "            indexesToDrop.append(index)\n",
    "        \n",
    "dfUrlTrue = dfUrlTrue.drop(indexesToDrop)\n",
    "dfUrlTrue = dfUrlTrue.reset_index(drop=True)\n",
    "\n",
    "dfUrlTrue = dfUrlTrue.sample(n=10000, random_state=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfUrlTrue.shape)\n",
    "print(dfUrlTrue.describe())\n",
    "print(dfUrlTrue.info())\n",
    "dfUrlTrue.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUrlTrue.sample(n=10).head(40).sort_values('url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_features.source_features import SourceFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceExtractor = SourceFeaturesExtractor(\"8c4b00a0873a12e53b7a2384ad8eeb9f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eu to deixando essa seção aqui pra testar rapidamente modificações no calculo das features..\n",
    "#Na hora da coleta tu sempre descobre algo que ta fazendo errado ou pode melhorar\n",
    "\n",
    "# url = \"https://www.correiodopovo.com.br/\"\n",
    "    \n",
    "# data_ipstack = sourceExtractor.get_ipstack_data(url)\n",
    "# print(data_ipstack)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# data_asn = sourceExtractor.get_asn_data(data_ipstack['subdomain_ip'])\n",
    "# print(data_asn)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# data_meta = sourceExtractor.get_ip_vs_asn_data(data_asn, data_ipstack)\n",
    "# print(data_meta)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# data_network = sourceExtractor.get_network_data(url)\n",
    "# print(data_network)\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceColumns = [\n",
    "                   'subdomain_ip', 'subdomain_ip_cc','subdomain_ip_is_brazil','subdomain_ip_is_us',\n",
    "                   'subdomain_ip_latitude','subdomain_ip_longitude',\n",
    "                   'subdomain_as_n','subdomain_as_cc',\n",
    "                   'subdomain_ipcc_equal_ascc',\n",
    "                   'domain_route_hops','domain_dns_caa_txt_count'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSourceFeatures = pd.DataFrame(columns = ['desinformacao_label', 'url'] + sourceColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando esse df a mais pq muitas urls compartilham as mesmas source features ja que sao do mesmo site\n",
    "#é imperativo: as features em data_network demoram 1 minuto POR INSTANCIA para serem coletadas\n",
    "    #300mil minutos recalculando vs 400 minutos reutilizando\n",
    "if os.path.isfile('dfSubdomainSourceFeatures22Apr2021.pkl'):\n",
    "    dfSubdomainSourceFeatures = pd.read_pickle('dfSubdomainSourceFeatures22Apr2021.pkl')\n",
    "else:  \n",
    "    dfSubdomainSourceFeatures = pd.DataFrame(columns = ['desinformacao_label','subdomain'] + sourceColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_source_features(dfUrlX, desinformacao_label, logFile):\n",
    "    newSubdomainCount = 0\n",
    "    begin = time.time()\n",
    "    \n",
    "    LOG_FILENAME =  'logSourceFeatures' + logFile + '.log'\n",
    "    logging.basicConfig(filename=LOG_FILENAME, filemode ='a',level=logging.WARNING)\n",
    "    \n",
    "    for index, row in dfUrlX.iterrows():\n",
    "\n",
    "        #Esse if ta aqui só pra não calcular pra todos os dados o tempo todo enquanto desenvolvia o nb\n",
    "        #Só comentar esse if pra percorrer todas as URLs calculando features\n",
    "        #if(index < 50):\n",
    "            url = row['url']\n",
    "#             with open(LOG_FILENAME) as myfile:\n",
    "#                 if url in myfile.read():\n",
    "#                     continue\n",
    "            dissectedUrl = tldextract.extract(url)\n",
    "            subdomain = '.'.join(part for part in dissectedUrl if part)\n",
    "\n",
    "            #print(\"Url:\", url)\n",
    "            #print(\"subdomain:\", subd)\n",
    "\n",
    "            dfSourceFeatures.at[len(dfSourceFeatures),'desinformacao_label'] = desinformacao_label\n",
    "            dfSourceFeatures.at[len(dfSourceFeatures)-1,'url'] = url\n",
    "\n",
    "            if(subdomain in list(dfSubdomainSourceFeatures['subdomain'])):\n",
    "                features = dfSubdomainSourceFeatures[dfSubdomainSourceFeatures['subdomain'] == subdomain][0:1]\n",
    "                for col in sourceColumns:\n",
    "                    dfSourceFeatures.at[len(dfSourceFeatures)-1,col] = features[0:1][col].values[0]\n",
    "            else:\n",
    "                \n",
    "                newSubdomainCount += 1\n",
    "                print(\"Coletando dominio inedito:\", subdomain, \"#\",newSubdomainCount)\n",
    "                print(\"Elapsed time: \", time.time() - begin)\n",
    "                try:\n",
    "                    #IPStack data vai ser calculado em cima do subdominio\n",
    "                    row_data_ipstack = sourceExtractor.get_ipstack_data(url)\n",
    "                    #ASN data vai ser calculado em cima do IP encontrado do SUBDOMINIO (IP de g1.globo.com e não globo.com)\n",
    "                    row_data_asn = sourceExtractor.get_asn_data(row_data_ipstack['subdomain_ip'])\n",
    "                    row_data_meta = sourceExtractor.get_ip_vs_asn_data(row_data_asn, row_data_ipstack)\n",
    "                    #Network data vai ser calculado em cima do DOMINIO (globo.com e não g1.globo.com)\n",
    "                    row_data_network = sourceExtractor.get_network_data(url)\n",
    "\n",
    "\n",
    "                    mergedDict = {**row_data_ipstack, **row_data_asn, **row_data_network,**row_data_meta}\n",
    "                    dfSubdomainSourceFeatures.at[len(dfSubdomainSourceFeatures),'desinformacao_label'] = desinformacao_label\n",
    "                    dfSubdomainSourceFeatures.at[len(dfSubdomainSourceFeatures)-1,'subdomain'] = subdomain\n",
    "\n",
    "                    for col in sourceColumns:\n",
    "                        dfSourceFeatures.at[len(dfSourceFeatures)-1,col] = mergedDict[col]\n",
    "                        dfSubdomainSourceFeatures.at[len(dfSubdomainSourceFeatures)-1,col] = mergedDict[col]\n",
    "                except Exception as e:\n",
    "                    logging.exception(url)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_source_features(dfUrlTrue,0,'urlTrue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfSourceFeatures.shape)\n",
    "print(dfSourceFeatures.describe())\n",
    "print(dfSourceFeatures.info())\n",
    "dfSourceFeatures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfSubdomainSourceFeatures.shape)\n",
    "print(dfSubdomainSourceFeatures.describe())\n",
    "print(dfSubdomainSourceFeatures.info())\n",
    "dfSubdomainSourceFeatures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_source_features(dfUrlFalse,1,'urlFalse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfSourceFeatures.shape)\n",
    "print(dfSourceFeatures.describe())\n",
    "print(dfSourceFeatures.info())\n",
    "dfSourceFeatures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfSubdomainSourceFeatures.shape)\n",
    "print(dfSubdomainSourceFeatures.describe())\n",
    "print(dfSubdomainSourceFeatures.info())\n",
    "dfSubdomainSourceFeatures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle no df\n",
    "dfSourceFeatures = dfSourceFeatures.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dfSourceFeatures.to_pickle(\"./dfSourceFeatures28Aug2021.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSubdomainSourceFeatures.to_pickle(\"./dfSubdomainSourceFeatures28Aug2021.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfSourceFeatures['desinformacao_label']= dfSourceFeatures['desinformacao_label'].astype('bool')\n",
    "# #dfSourceFeatures['url']= dfSourceFeatures['url'].astype('')\n",
    "\n",
    "# #dfSourceFeatures['subdomain_ip']= dfSourceFeatures['subdomain_ip'].astype('')\n",
    "# dfSourceFeatures['subdomain_ip_cc']= dfSourceFeatures['subdomain_ip_cc'].astype('category')\n",
    "# dfSourceFeatures['subdomain_ip_is_brazil']= dfSourceFeatures['subdomain_ip_is_brazil'].astype('bool')\n",
    "# dfSourceFeatures['subdomain_ip_is_us']= dfSourceFeatures['subdomain_ip_is_us'].astype('bool')\n",
    "# dfSourceFeatures['subdomain_ip_latitude']= dfSourceFeatures['subdomain_ip_latitude'].astype('float')\n",
    "# dfSourceFeatures['subdomain_ip_longitude']= dfSourceFeatures['subdomain_ip_longitude'].astype('float')\n",
    "# #dfSourceFeatures['subdomain_as_n']= dfSourceFeatures['subdomain_as_n'].astype('')\n",
    "# dfSourceFeatures['subdomain_as_cc']= dfSourceFeatures['subdomain_as_cc'].astype('category')\n",
    "# dfSourceFeatures['subdomain_ipcc_equal_ascc']= dfSourceFeatures['subdomain_ipcc_equal_ascc'].astype('bool')\n",
    "# dfSourceFeatures['domain_route_hops']= dfSourceFeatures['domain_route_hops'].astype('int')\n",
    "# dfSourceFeatures['domain_dns_caa_txt_count']= dfSourceFeatures['domain_dns_caa_txt_count'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dfSubdomainSourceFeatures['subdomain']= dfSubdomainSourceFeatures['subdomain'].astype('')\n",
    "\n",
    "# #dfSubdomainSourceFeatures['subdomain_ip']= dfSubdomainSourceFeatures['subdomain_ip'].astype('')\n",
    "# dfSubdomainSourceFeatures['subdomain_ip_cc']= dfSubdomainSourceFeatures['subdomain_ip_cc'].astype('category')\n",
    "# dfSubdomainSourceFeatures['subdomain_ip_is_brazil']= dfSubdomainSourceFeatures['subdomain_ip_is_brazil'].astype('bool')\n",
    "# dfSubdomainSourceFeatures['subdomain_ip_is_us']= dfSubdomainSourceFeatures['subdomain_ip_is_us'].astype('bool')\n",
    "# dfSubdomainSourceFeatures['subdomain_ip_latitude']= dfSubdomainSourceFeatures['subdomain_ip_latitude'].astype('float')\n",
    "# dfSubdomainSourceFeatures['subdomain_ip_longitude']= dfSubdomainSourceFeatures['subdomain_ip_longitude'].astype('float')\n",
    "# #dfSubdomainSourceFeatures['subdomain_as_n']= dfSubdomainSourceFeatures['subdomain_as_n'].astype('')\n",
    "# dfSubdomainSourceFeatures['subdomain_as_cc']= dfSubdomainSourceFeatures['subdomain_as_cc'].astype('category')\n",
    "# dfSubdomainSourceFeatures['subdomain_ipcc_equal_ascc']= dfSubdomainSourceFeatures['subdomain_ipcc_equal_ascc'].astype('bool')\n",
    "# dfSubdomainSourceFeatures['domain_route_hops']= dfSubdomainSourceFeatures['domain_route_hops'].astype('int')\n",
    "# dfSubdomainSourceFeatures['domain_dns_caa_txt_count']= dfSubdomainSourceFeatures['domain_dns_caa_txt_count'].astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sandbox de EDA, ignorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(dfSourceFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSubdomainSourceFeatures.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSubdomainSourceFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSourceFeatures.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSubdomainSourceFeatures[dfSubdomainSourceFeatures['desinformacao_label'] == 1]['domain_dns_caa_txt_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aCategorical = [\n",
    "                   'subdomain_ip', 'subdomain_ip_cc',\n",
    "                   'subdomain_as_n','subdomain_as_cc',\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aNumerical = [\n",
    "'subdomain_ip_is_brazil','subdomain_ip_is_us',\n",
    "                   'subdomain_ip_latitude','subdomain_ip_longitude',\n",
    "           \n",
    "                   'subdomain_ipcc_equal_ascc',\n",
    "                   'domain_route_hops','domain_dns_caa_txt_count'\n",
    "                 ]\n",
    "for feature in aNumerical:\n",
    "    print(\"Feature:\", feature)\n",
    "    print(\"Media true:\")\n",
    "    print(dfSubdomainSourceFeatures[dfSubdomainSourceFeatures['desinformacao_label'] == 0][feature].mean())\n",
    "    print(\"Media false:\")\n",
    "    print(dfSubdomainSourceFeatures[dfSubdomainSourceFeatures['desinformacao_label'] == 1][feature].mean())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allSubs = []\n",
    "# for index, row in dfUrlTrue.iterrows():\n",
    "#     url = row['url']\n",
    "#     dissectedUrl = tldextract.extract(url)\n",
    "#     subdomain = '.'.join(part for part in dissectedUrl if len(part)>0)\n",
    "#     allSubs.append(subdomain)\n",
    "#     #print(len(allSubs))\n",
    "# print(len(list(set(allSubs))))\n",
    "\n",
    "# allSubs = []\n",
    "# for index, row in dfUrlFalse.iterrows():\n",
    "#     url = row['url']\n",
    "#     dissectedUrl = tldextract.extract(url)\n",
    "#     subdomain = '.'.join(part for part in dissectedUrl if len(part)>0)\n",
    "#     allSubs.append(subdomain)\n",
    "#     #print(len(allSubs))\n",
    "# print(len(list(set(allSubs))))\n",
    "\n",
    "# #Tem 108 sites true\n",
    "# #292 sites fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lendo_dataset(localizacao_arquivo):\n",
    "#     # Leitura de dados:\n",
    "#     dados = []\n",
    "#     with open(localizacao_arquivo, mode='r') as f:\n",
    "#         for line in f:\n",
    "#             dados.append(json.loads(line.strip()))\n",
    "\n",
    "#     # Imprimindo primeiro 3 coletas de cada\n",
    "#     for i in range(3):\n",
    "#         pp = pprint.PrettyPrinter(indent=4)\n",
    "#         pp.pprint(dados[i])\n",
    "#         print('\\n')\n",
    "\n",
    "\n",
    "# # Lendo TrueNews\n",
    "# print('\\n---------------TRUE NEWS:\\n')\n",
    "# lendo_dataset('data/DATASET_MPMG-TrueNews_selected.txt')\n",
    "\n",
    "# print('\\n\\n\\n---------------FAKE NEWS:\\n')\n",
    "# # Lendo FakeNews\n",
    "# lendo_dataset('data/DATASET_MPMG-FakeNews_matched.txt')\n",
    "# dfTrue = pd.read_json('data/DATASET_MPMG-TrueNews_selected.json')\n",
    "# dfFalse = pd.read_json('data/DATASET_MPMG-FakeNews_matched.json')\n",
    "# print(dfTrue.columns)\n",
    "\n",
    "# dfTrue.replace('', np.nan, inplace=True)\n",
    "# dfFalse.replace('', np.nan, inplace=True)\n",
    "# dfTrue.replace('NULL', np.nan, inplace=True)\n",
    "# dfFalse.replace('NULL', np.nan, inplace=True)\n",
    "#print(dfTrue.shape, dfFalse.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
